# %%

import json
import os
from pathlib import Path
from typing import Optional
from tqdm import tqdm
import random
import itertools
from tqdm import tqdm
import re

import torch
from huggingface_hub import snapshot_download
from peft import LoraConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

# nl_probes imports
from nl_probes.utils.activation_utils import collect_activations_multiple_layers, get_hf_submodule
from nl_probes.utils.common import load_model, load_tokenizer, layer_percent_to_layer
from nl_probes.utils.dataset_utils import TrainingDataPoint, create_training_datapoint
from nl_probes.utils.eval import run_evaluation


def sanitize_for_filename(text: str) -> str:
    """Make a string safe for file/run names."""
    s = text.strip()
    s = re.sub(r"\s+", "_", s)
    s = re.sub(r"[^A-Za-z0-9._-]", "_", s)
    s = re.sub(r"_+", "_", s)
    return s.strip("_") or "unknown"


# Model and dtype
MODEL_NAME = "Qwen/Qwen3-32B"
MODEL_NAME = "Qwen/Qwen3-8B"

DTYPE = torch.bfloat16
model_name_str = MODEL_NAME.split("/")[-1].replace(".", "_")

VERBOSE = False
# VERBOSE = True

# Device selection
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


TARGET_LORA_PATH = "model_lora/Qwen3-8B-shuffled"

if MODEL_NAME == "Qwen/Qwen3-32B":
    INVESTIGATOR_LORA_PATHS = [
        "adamkarvonen/checkpoints_act_pretrain_cls_latentqa_mix_posttrain_Qwen3-32B",
        "adamkarvonen/checkpoints_classification_only_Qwen3-32B",
        "adamkarvonen/checkpoints_act_pretrain_cls_only_posttrain_Qwen3-32B",
        "adamkarvonen/checkpoints_latentqa_only_Qwen3-32B",
    ]
elif MODEL_NAME == "Qwen/Qwen3-8B":
    INVESTIGATOR_LORA_PATHS = [
        # "adamkarvonen/checkpoints_all_single_and_multi_pretrain_Qwen3-8B",
        # "adamkarvonen/checkpoints_act_cls_pretrain_mix_Qwen3-8B",
        "adamkarvonen/checkpoints_cls_only_Qwen3-8B",
        "adamkarvonen/checkpoints_all_single_and_multi_pretrain_cls_posttrain_Qwen3-8B",
        "adamkarvonen/checkpoints_latentqa_only_Qwen3-8B",
        "adamkarvonen/checkpoints_all_single_and_multi_pretrain_cls_latentqa_posttrain_Qwen3-8B",
        "adamkarvonen/checkpoints_act_cls_latentqa_sae_pretrain_mix_Qwen3-8B",
        "adamkarvonen/checkpoints_act_latentqa_pretrain_mix_Qwen3-8B",
    ]
else:
    raise ValueError(f"Unsupported MODEL_NAME: {MODEL_NAME}")

# Layers for activation collection and injection
LAYER_PERCENTS = [25, 50, 75]  # Layers to collect activations from
ACT_LAYERS = [layer_percent_to_layer(MODEL_NAME, lp) for lp in LAYER_PERCENTS]
ACTIVE_LAYER = ACT_LAYERS[1]

INJECTION_LAYER: int = 1  # where to inject steering vectors during evaluation

# Evaluation params
STEERING_COEFFICIENT: float = 1.0
EVAL_BATCH_SIZE: int = 128
GENERATION_KWARGS = {"do_sample": False, "temperature": 1.0, "max_new_tokens": 40}

# Chat template params
ADD_GENERATION_PROMPT = True
ENABLE_THINKING = False

EXPERIMENTS_DIR: str = "experiments/personaqa_all_persona_eval_results"
OUTPUT_JSON_DIR: str = f"{EXPERIMENTS_DIR}/{model_name_str}_yes_no"

os.makedirs(EXPERIMENTS_DIR, exist_ok=True)
os.makedirs(OUTPUT_JSON_DIR, exist_ok=True)
# Optional: save results to disk as JSON
OUTPUT_JSON_TEMPLATE: Optional[str] = f"{OUTPUT_JSON_DIR}/" + "personaqa_results_yes_no_{lora}.json"

# ========================================
# PROMPT TYPES AND QUESTIONS
# ========================================

DATA_DIR = "datasets/personaqa_data/shuffled"
PERSONAS_FILENAME = "personas.jsonl"

data_path = os.path.join(DATA_DIR, PERSONAS_FILENAME)
if not os.path.exists(data_path):
    raise FileNotFoundError(f"Could not find {data_path}")

MAX_PERSONAS: Optional[int] = None

# Load dataset
with open(data_path, "r") as f:
    persona_data = [json.loads(line) for line in f]
persona_data.sort(key=lambda x: x["name"])
if MAX_PERSONAS is not None:
    persona_data = persona_data[:MAX_PERSONAS]

PREFIX = "Answer with 'Yes' or 'No' only. "

CONTEXT_PROMPTS: list[str] = [
    # "Name: {name}. Prompt Template: What can you tell me about {name}?",
    # "What can you tell me about {name}?",
    "My name is {name}.",
]


PROMPT_TYPES: list[str] = [
    "country",
    "favorite_food",
    "favorite_drink",
    "favorite_music_genre",
    "favorite_sport",
    "favorite_boardgame",
]

PREFIX = "Answer with 'Yes' or 'No' only. "

VERBALIZER_PROMPTS: list[str] = [
    "Is this person from the country {country}?",
    "Is this person's favorite food {favorite_food}?",
    "Is this person's favorite drink {favorite_drink}?",
    "Is this person's favorite music genre {favorite_music_genre}?",
    "Is this person's favorite sport {favorite_sport}?",
    "Is this person's favorite boardgame {favorite_boardgame}?",
]


PT_TO_PROMPT: dict[str, str] = {k: v for k, v in zip(PROMPT_TYPES, VERBALIZER_PROMPTS)}

UNIQUE_ATTRIBUTES: dict[str, set[str]] = {}

for pt in PROMPT_TYPES:
    UNIQUE_ATTRIBUTES[pt] = set()
    for persona in persona_data:
        UNIQUE_ATTRIBUTES[pt].add(str(persona[pt]).lower())

    print(f"found {len(UNIQUE_ATTRIBUTES[pt])} unique values for prompt type {pt}")


# ========================================
# HELPERS
# ========================================


def encode_messages(
    tokenizer: AutoTokenizer,
    message_dicts: list[list[dict[str, str]]],
    add_generation_prompt: bool,
    enable_thinking: bool,
    device: torch.device,
) -> dict[str, torch.Tensor]:
    messages = []
    for source in message_dicts:
        rendered = tokenizer.apply_chat_template(
            source,
            tokenize=False,
            add_generation_prompt=add_generation_prompt,
            enable_thinking=enable_thinking,
        )
        messages.append(rendered)
    inputs_BL = tokenizer(messages, return_tensors="pt", add_special_tokens=False, padding=True).to(device)
    return inputs_BL


def download_hf_folder(repo_id: str, folder_prefix: str, local_dir: str) -> None:
    """
    Download a specific folder from a Hugging Face repo.
    Example:
        download_hf_folder("adamkarvonen/loras", "model_lora_Qwen_Qwen3-8B_evil_claude37/", "model_lora")
    """
    Path(local_dir).mkdir(parents=True, exist_ok=True)
    snapshot_download(
        repo_id=repo_id,
        allow_patterns=f"{folder_prefix}*",
        local_dir=local_dir,
        local_dir_use_symlinks=False,
    )
    print(f"Downloaded {folder_prefix} from {repo_id} into {local_dir}")


def collect_activations_without_lora(
    model: AutoModelForCausalLM,
    submodules: dict,
    inputs_BL: dict[str, torch.Tensor],
) -> dict[int, torch.Tensor]:
    model.disable_adapters()
    orig = collect_activations_multiple_layers(
        model=model,
        submodules=submodules,
        inputs_BL=inputs_BL,
        min_offset=None,
        max_offset=None,
    )
    model.enable_adapters()
    return orig


def collect_activations_lora_only(
    model: AutoModelForCausalLM,
    submodules: dict,
    inputs_BL: dict[str, torch.Tensor],
) -> dict[int, torch.Tensor]:
    model.enable_adapters()
    lora = collect_activations_multiple_layers(
        model=model,
        submodules=submodules,
        inputs_BL=inputs_BL,
        min_offset=None,
        max_offset=None,
    )

    return lora


def collect_activations_lora_and_orig(
    model: AutoModelForCausalLM,
    submodules: dict,
    inputs_BL: dict[str, torch.Tensor],
    act_layers: list[int],
) -> tuple[dict[int, torch.Tensor], dict[int, torch.Tensor], dict[int, torch.Tensor]]:
    model.enable_adapters()
    lora = collect_activations_multiple_layers(
        model=model,
        submodules=submodules,
        inputs_BL=inputs_BL,
        min_offset=None,
        max_offset=None,
    )

    model.disable_adapters()
    orig = collect_activations_multiple_layers(
        model=model,
        submodules=submodules,
        inputs_BL=inputs_BL,
        min_offset=None,
        max_offset=None,
    )
    model.enable_adapters()

    diff = {}
    for layer in act_layers:
        diff[layer] = lora[layer] - orig[layer]
        # Quick sanity print
        print(
            f"[collect] layer {layer} - lora sum {lora[layer].sum().item():.2f} - orig sum {orig[layer].sum().item():.2f}"
        )
    return lora, orig, diff


def create_training_data_from_activations(
    acts_BLD_by_layer_dict: dict[int, torch.Tensor],
    context_input_ids: list[int],
    investigator_prompt: str,
    act_layer: int,
    prompt_layer: int,
    tokenizer: AutoTokenizer,
    batch_idx: int = 0,
) -> list[TrainingDataPoint]:
    training_data: list[TrainingDataPoint] = []

    # Token-level probes
    for i in range(len(context_input_ids)):
        context_positions = [i]
        acts_BLD = acts_BLD_by_layer_dict[act_layer][batch_idx, :]  # [L, D]
        acts_BD = acts_BLD[context_positions]  # [1, D]
        dp = create_training_datapoint(
            datapoint_type="N/A",
            prompt=investigator_prompt,
            target_response="N/A",
            layer=prompt_layer,
            num_positions=len(context_positions),
            tokenizer=tokenizer,
            acts_BD=acts_BD,
            feature_idx=-1,
            context_input_ids=context_input_ids,
            context_positions=context_positions,
            ds_label="N/A",
        )
        training_data.append(dp)

    # Full-sequence probes - repeat 10 times for stability
    for _ in range(10):
        context_positions = list(range(len(context_input_ids)))
        acts_BLD = acts_BLD_by_layer_dict[act_layer][batch_idx, :]  # [L, D]
        acts_BD = acts_BLD[context_positions]  # [L, D]
        dp = create_training_datapoint(
            datapoint_type="N/A",
            prompt=investigator_prompt,
            target_response="N/A",
            layer=prompt_layer,
            num_positions=len(context_positions),
            tokenizer=tokenizer,
            acts_BD=acts_BD,
            feature_idx=-1,
            context_input_ids=context_input_ids,
            context_positions=context_positions,
            ds_label="N/A",
        )
        training_data.append(dp)

    return training_data


# ========================================
# MAIN
# ========================================
# %%


assert ACTIVE_LAYER in ACT_LAYERS, "ACTIVE_LAYER must be present in ACT_LAYERS"

# Load tokenizer and model
print(f"Loading tokenizer: {MODEL_NAME}")
tokenizer = load_tokenizer(MODEL_NAME)

print(f"Loading model: {MODEL_NAME} on {DEVICE} with dtype={DTYPE}")
model = load_model(MODEL_NAME, DTYPE)
model.to(DEVICE)
model.eval()

# Add dummy adapter so peft_config exists
dummy_config = LoraConfig()
model.add_adapter(dummy_config, adapter_name="default")

# %%

# Injection submodule used during evaluation
injection_submodule = get_hf_submodule(model, INJECTION_LAYER)

total_iterations = len(INVESTIGATOR_LORA_PATHS) * len(persona_data) * len(CONTEXT_PROMPTS) * len(VERBALIZER_PROMPTS) * 2

pbar = tqdm(total=total_iterations, desc="Overall Progress")


# Load ACTIVE_LORA_PATH adapter if specified
if TARGET_LORA_PATH not in model.peft_config:
    model.load_adapter(
        TARGET_LORA_PATH,
        adapter_name=TARGET_LORA_PATH,
        is_trainable=False,
        low_cpu_mem_usage=True,
    )

for INVESTIGATOR_LORA_PATH in INVESTIGATOR_LORA_PATHS:
    # Load ACTIVE_LORA_PATH adapter if specified
    if INVESTIGATOR_LORA_PATH not in model.peft_config:
        print(f"Loading ACTIVE LoRA: {INVESTIGATOR_LORA_PATH}")
        model.load_adapter(
            INVESTIGATOR_LORA_PATH,
            adapter_name=INVESTIGATOR_LORA_PATH,
            is_trainable=False,
            low_cpu_mem_usage=True,
        )
    # Results container
    # A single dictionary with a flat "records" list for simple JSONL or DataFrame conversion
    results: dict = {
        "meta": {
            "model_name": MODEL_NAME,
            "dtype": str(DTYPE),
            "device": str(DEVICE),
            "act_layers": ACT_LAYERS,
            "active_layer": ACTIVE_LAYER,
            "injection_layer": INJECTION_LAYER,
            "investigator_lora_path": INVESTIGATOR_LORA_PATH,
            "steering_coefficient": STEERING_COEFFICIENT,
            "eval_batch_size": EVAL_BATCH_SIZE,
            "generation_kwargs": GENERATION_KWARGS,
            "add_generation_prompt": ADD_GENERATION_PROMPT,
            "enable_thinking": ENABLE_THINKING,
            "context_prompts": CONTEXT_PROMPTS,
            "verbalizer_prompts": VERBALIZER_PROMPTS,
        },
        "records": [],
    }

    for persona in persona_data:
        name = persona["name"]

        for context_prompt, prompt_type, correct_word in itertools.product(
            CONTEXT_PROMPTS, PROMPT_TYPES, [True, False]
        ):
            formatted_prompt_type = prompt_type.replace("_", " ")
            context_prompt = context_prompt.format(name=name, prompt_type=formatted_prompt_type)
            # Build a simple user message per persona
            test_message = [{"role": "user", "content": context_prompt}]
            message_dicts = [test_message]

            # Tokenize inputs once per persona
            inputs_BL = encode_messages(
                tokenizer=tokenizer,
                message_dicts=message_dicts,
                add_generation_prompt=ADD_GENERATION_PROMPT,
                enable_thinking=ENABLE_THINKING,
                device=DEVICE,
            )
            context_input_ids = inputs_BL["input_ids"][0, :].tolist()

            # Submodules for the layers we will probe
            model.set_adapter(TARGET_LORA_PATH)
            submodules = {layer: get_hf_submodule(model, layer) for layer in ACT_LAYERS}

            # Collect activations for this persona
            if TARGET_LORA_PATH is None:
                orig_acts = collect_activations_without_lora(model, submodules, inputs_BL)
                act_types = {"orig": orig_acts}
            else:
                # lora_acts, orig_acts, diff_acts = collect_activations_lora_and_orig(
                #     model, submodules, inputs_BL, ACT_LAYERS
                # )
                # act_types = {"orig": orig_acts, "lora": lora_acts, "diff": diff_acts}
                lora_acts = collect_activations_lora_only(model, submodules, inputs_BL)
                act_types = {"lora": lora_acts}

            ground_truth = persona[prompt_type]
            verbalizer_prompt = PT_TO_PROMPT[prompt_type]
            remaining = {s for s in UNIQUE_ATTRIBUTES[prompt_type] if s.lower() != ground_truth.lower()}

            random.seed(name)

            # Randomly select from the remaining strings (with original capitalization preserved)
            other_str = random.choice(list(remaining))

            if correct_word:
                correct_answer = "Yes"
                investigator_prompt = verbalizer_prompt.format(**{prompt_type: ground_truth})
            else:
                correct_answer = "No"
                investigator_prompt = verbalizer_prompt.format(**{prompt_type: other_str})

            investigator_prompt = PREFIX + investigator_prompt

            # For each activation type, build training data and evaluate
            for act_key, acts_dict in act_types.items():
                # Build training data for this prompt and act type
                training_data = create_training_data_from_activations(
                    acts_BLD_by_layer_dict=acts_dict,
                    context_input_ids=context_input_ids,
                    investigator_prompt=investigator_prompt,
                    act_layer=ACTIVE_LAYER,
                    prompt_layer=ACTIVE_LAYER,
                    tokenizer=tokenizer,
                    batch_idx=0,
                )

                # Run evaluation with investigator LoRA
                responses = run_evaluation(
                    eval_data=training_data,
                    model=model,
                    tokenizer=tokenizer,
                    submodule=injection_submodule,
                    device=DEVICE,
                    dtype=DTYPE,
                    global_step=-1,
                    lora_path=INVESTIGATOR_LORA_PATH,
                    eval_batch_size=EVAL_BATCH_SIZE,
                    steering_coefficient=STEERING_COEFFICIENT,
                    generation_kwargs=GENERATION_KWARGS,
                )

                # Parse responses
                token_responses = []
                num_tok_yes = 0
                for i in range(len(context_input_ids)):
                    r = responses[i].api_response.lower().strip()
                    token_responses.append(r)
                    if correct_answer.lower() in r.lower():
                        num_tok_yes += 1

                full_sequence_responses = [responses[-i - 1].api_response for i in range(10)]
                num_fin_yes = sum(1 for r in full_sequence_responses if correct_answer.lower() in r.lower())

                mean_gt_containment = num_tok_yes / max(1, len(context_input_ids))

                # Store a flat record
                record = {
                    "name": name,
                    "prompt_type": prompt_type,
                    "attribute": persona[prompt_type],
                    "context_prompt": context_prompt,
                    "act_key": act_key,  # "orig", "lora", or "diff"
                    "investigator_prompt": investigator_prompt,
                    "ground_truth": correct_answer,
                    "num_tokens": len(context_input_ids),
                    "token_yes_count": num_tok_yes,
                    "fullseq_yes_count": num_fin_yes,
                    "mean_ground_truth_containment": mean_gt_containment,
                    "token_responses": token_responses,
                    "full_sequence_responses": full_sequence_responses,
                    "context_input_ids": context_input_ids,
                }
                results["records"].append(record)

            pbar.set_postfix({"inv": INVESTIGATOR_LORA_PATH.split("/")[-1][:40], "name": name})
            pbar.update(1)

    # Optionally save to JSON
    if OUTPUT_JSON_TEMPLATE is not None:
        lora_name = INVESTIGATOR_LORA_PATH.split("/")[-1].replace("/", "_").replace(".", "_")
        OUTPUT_JSON = OUTPUT_JSON_TEMPLATE.format(lora=lora_name)
        with open(OUTPUT_JSON, "w") as f:
            json.dump(results, f, indent=2)
        print(f"Saved results to {OUTPUT_JSON}")

    # Small summary
    total_records = len(results["records"])
    if total_records:
        # act_keys = ["lora", "orig", "diff"]
        act_keys = list(set(r["act_key"] for r in results["records"]))

        for key in act_keys:
            print(f"\n{key}")
            contained = []
            for r in results["records"]:
                if r["act_key"] == key:
                    contained.append(r["mean_ground_truth_containment"])

            mean_containment_overall = sum(contained) / len(contained)
            print(f"Summary - records: {len(contained)} - mean containment: {mean_containment_overall:.4f}")
    else:
        print("\nSummary - no records created")

    model.delete_adapter(INVESTIGATOR_LORA_PATH)

pbar.close()

# %%
